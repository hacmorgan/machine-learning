#!/usr/bin/env python

import numpy as np
import math
import argparse
import sys
import time
import json


'''
todo
- '--hidden-layer-sizes' input arg: comma separated list of hidden layer sizes
  - will determine the value of shapes parameter
'''


img_width = 64
img_height = 48


def costs(data_dict, network):
    for X, y, cost_list in (
            (data_dict["X_train"], data_dict["y_train"], network["accuracies"]["train_costs"]),
            (data_dict["X_val"], data_dict["y_val"], network["accuracies"]["val_costs"])
    ):
        m = len(y)
        activations, zeds = fwd_prop(X, network)
        h = activations[-1]
        regterm = 0
        for theta in network["thetas"]:
            regterm += np.sum(theta[1:,:] ** 2)
        cost_list.append( 1.0/m * np.sum( -y * np.log(h) - (1 - y) * np.log(1-h) ) +
                      float(network["hyperparams"]["lmbda"])/(2*m) * regterm )
    return


def theta_grads(X, y, network):
    # initialisation
    m = len(y)
    for i, shape in enumerate(network["shapes"]):
        network["theta_grads"][i] = np.zeros(shape)
    # forwardprop
    activations, zeds = fwd_prop(X, network)
    # backprop
    for i in range(len(network["thetas"])-1, -1, -1):
        if i == len(network["thetas"])-1:
            delta = activations[-1] - y
        else:
            delta = np.matmul(prev_delta, prev_theta[1:,:].transpose()) * sigmoid_gradient(zeds[i+1]) # we add 1 because activations has the input activations as wel as those from each layer of the network
        network["theta_grads"][i] = (1/m * np.matmul(activations[i].transpose(), delta))
        theta_reg = network["hyperparams"]["lmbda"]/m * network["thetas"][i]
        theta_reg[0,:] = 0
        network["theta_grads"][i] += theta_reg 
        prev_delta = delta
        prev_theta = network["thetas"][i]
    return


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def sigmoid_gradient(z):
    return sigmoid(z) * (1 - sigmoid(z))


def fwd_prop(X, network):
    m = len(X)
    activations = [np.concatenate( (np.ones((m,1)),X), axis=1 )]
    zeds = []
    for theta in network["thetas"]:
        zeds.append( np.matmul(activations[-1], theta) )
        activations.append( np.concatenate((np.ones((m,1)), sigmoid(zeds[-1])), axis=1) )
    activations[-1] = np.delete(activations[-1], 0, 1) 
    return activations, zeds


def initialise_weights(network):
    for shape in network["shapes"]:
        network["thetas"].append( np.random.normal(loc=0, scale=(1/math.sqrt(shape[0])), size=shape) )
        network["theta_grads"].append(np.zeros(shape))
    return


# return a dictionary with shuffled and segmented data
def get_data(filename):
    # read data from file
    with open(filename, "rb") as datafile:
        all_data = np.frombuffer(datafile.read(), dtype=np.uint8)
    all_data = all_data.astype(np.float32)
    num_records = int(np.shape(all_data)[0] / (img_height*img_width+1))
    # reformat and scale the data
    formatted_data = all_data.reshape((num_records, img_height*img_width+1))
    scaling_matrix = np.ones(formatted_data.shape, dtype = np.float32) * 255
    scaling_matrix[:,-1] = 1
    formatted_data /= scaling_matrix
    # shuffle and put in dictionary
    np.random.shuffle(formatted_data)
    train_index = int(0.6 * len(formatted_data))
    val_index = int(0.8 * len(formatted_data))
    data_dict = {
        "X_train" : formatted_data[:train_index, :-1],
        "y_train" : formatted_data[:train_index, -1].reshape((train_index,1)),
        "X_val" : formatted_data[train_index:val_index, :-1],
        "y_val" : formatted_data[train_index:val_index, -1].reshape((val_index-train_index,1)),
        "X_test" : formatted_data[val_index:, :-1],
        "y_test" : formatted_data[val_index:, -1].reshape((len(formatted_data)-val_index,1))
    }
    return data_dict


def sgd(network, data_dict, no_improvement_in=10):
    training_data = np.concatenate((data_dict["X_train"], data_dict["y_train"]), axis=1)
    m = len(training_data)
    while True:
        np.random.shuffle(training_data)
        X = training_data[:,:-1]
        y = training_data[:,-1].reshape((len(training_data),1))
        theta_grads(X, y, network)
        # update weights
        for i, theta_grad in enumerate(network["theta_grads"]):
            network["thetas"][i] -= network["hyperparams"]["eta"] / m * theta_grad
        costs(data_dict, network)
        test_accuracy(X, y, network)
        num_correct = network["accuracies"]["num_correct"][-no_improvement_in:]
        if len(num_correct) == no_improvement_in and num_correct.count(num_correct[-1]) == no_improvement_in:
            print("eta: " + str(network["hyperparams"]["eta"]) + ", "
                  "lambda: " + str(network["hyperparams"]["lmbda"]) + ", "
                  "num correct: " + str(network["accuracies"]["num_correct"][-1]))
            return
                                   

def test_accuracy(X, y, network):
    activations, zeds = fwd_prop(X, network)
    h = activations[-1]
    # define num_examples if not yet done
    if network["accuracies"]["num_examples"] == 0:
        network["accuracies"]["num_examples"] = len(h)
    num_examples = network["accuracies"]["num_examples"]
    num_correct = num_examples - np.sum( (y-np.round(h)) ** 2 )
    network["accuracies"]["num_correct"].append(num_correct)
    perc_correct = num_correct / num_examples
    global verbose
    if verbose:
        print("Correctly classified " + str(num_correct) + 
              " out of " + str(num_examples) + " examples, which is an accuracy of " +
              str(perc_correct * 100) + "%")


##############
#    MAIN    #
##############
def get_args():
    parser = argparse.ArgumentParser(description='train a simple 3 layer neural network on given data')
    parser.add_argument('--eta', type=float, default=1, help='learning rate') 
    parser.add_argument('--lmbda', type=float, default=3, help='regularization parameter') 
    parser.add_argument('--no-improvement-in', type=int, default=50, help='number of successive epochs without improvement before considering training over') 
    parser.add_argument('--training-data', '-t', type=str, required=True, help='file containing training data')
    parser.add_argument('--verbose', '-v', action='store_true')
    return parser.parse_args()

def main(args):
    # set verbose flag
    global verbose
    verbose = args.verbose
    # initialise network dictionary
    hidden_units = 30
    network = {
        "hyperparams" : { "lmbda" : args.lmbda,
                          "eta" : args.eta },
        "shapes" : ( (img_width*img_height+1, hidden_units), (hidden_units+1, 1) ),
        "thetas" : [],
        "theta_grads" : [],
        "accuracies" : { "num_examples" : 0,
                         "num_correct" : [],
                         "val_costs" : [],
                         "train_costs" : []}
    }
    initialise_weights(network)
    data_dict = get_data(args.training_data)
    # perform gradient descent
    sgd(network, data_dict, args.no_improvement_in)
    for i in range(len(network["thetas"])):
        network["thetas"][i] = network["thetas"][i].tolist()
        network["theta_grads"][i] = network["theta_grads"][i].tolist()
    save_string = '_'.join(["lmbda", str(network["hyperparams"]["lmbda"]),
                            "eta", str(network["hyperparams"]["eta"]),
                            "no-imp-in", str(args.no_improvement_in),
                            ".json"])
    with open(save_string, "w") as jsonfile:
        json.dump(network, jsonfile)
    return
    
if __name__ == '__main__':
    sys.exit(main(get_args()))
    
